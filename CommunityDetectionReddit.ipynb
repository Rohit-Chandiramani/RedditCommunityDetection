{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8CvV2Z3MSHaR",
        "outputId": "fabcdc2f-898d-4ae8-fd73-176d700afa0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorch\n",
            "  Downloading pytorch-1.0.2.tar.gz (689 bytes)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.18.0-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
            "Collecting torchaudio\n",
            "  Downloading torchaudio-2.3.0-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: numpy in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torchvision) (1.24.4)\n",
            "Requirement already satisfied: torch==2.3.0 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torchvision) (2.3.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torchvision) (10.2.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch==2.3.0->torchvision) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch==2.3.0->torchvision) (4.8.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch==2.3.0->torchvision) (1.12)\n",
            "Requirement already satisfied: networkx in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch==2.3.0->torchvision) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch==2.3.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch==2.3.0->torchvision) (2024.2.0)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch==2.3.0->torchvision) (2021.4.0)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0->torchvision) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch==2.3.0->torchvision) (2021.12.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from jinja2->torch==2.3.0->torchvision) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from sympy->torch==2.3.0->torchvision) (1.3.0)\n",
            "Downloading torchvision-0.18.0-cp311-cp311-win_amd64.whl (1.2 MB)\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "   ---------------------------------------- 0.0/1.2 MB ? eta -:--:--\n",
            "    --------------------------------------- 0.0/1.2 MB 682.7 kB/s eta 0:00:02\n",
            "   - -------------------------------------- 0.0/1.2 MB 393.8 kB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 0.1/1.2 MB 901.1 kB/s eta 0:00:02\n",
            "   -------- ------------------------------- 0.2/1.2 MB 1.3 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 0.4/1.2 MB 1.9 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 0.6/1.2 MB 2.3 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 0.7/1.2 MB 2.4 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 0.9/1.2 MB 2.7 MB/s eta 0:00:01\n",
            "   ----------------------------------- ---- 1.0/1.2 MB 2.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 1.2/1.2 MB 2.7 MB/s eta 0:00:00\n",
            "Downloading torchaudio-2.3.0-cp311-cp311-win_amd64.whl (2.4 MB)\n",
            "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.2/2.4 MB 5.4 MB/s eta 0:00:01\n",
            "   ------- -------------------------------- 0.5/2.4 MB 4.9 MB/s eta 0:00:01\n",
            "   ---------- ----------------------------- 0.6/2.4 MB 5.1 MB/s eta 0:00:01\n",
            "   -------------- ------------------------- 0.9/2.4 MB 4.9 MB/s eta 0:00:01\n",
            "   ----------------- ---------------------- 1.1/2.4 MB 4.8 MB/s eta 0:00:01\n",
            "   --------------------- ------------------ 1.2/2.4 MB 4.7 MB/s eta 0:00:01\n",
            "   ---------------------- ----------------- 1.4/2.4 MB 4.3 MB/s eta 0:00:01\n",
            "   ------------------------- -------------- 1.5/2.4 MB 4.1 MB/s eta 0:00:01\n",
            "   ----------------------------- ---------- 1.8/2.4 MB 4.3 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 1.9/2.4 MB 4.2 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 2.2/2.4 MB 4.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 2.3/2.4 MB 4.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 2.4/2.4 MB 4.0 MB/s eta 0:00:00\n",
            "Building wheels for collected packages: pytorch\n",
            "  Building wheel for pytorch (setup.py): started\n",
            "  Building wheel for pytorch (setup.py): finished with status 'error'\n",
            "  Running setup.py clean for pytorch\n",
            "Failed to build pytorch\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py bdist_wheel did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [6 lines of output]\n",
            "      Traceback (most recent call last):\n",
            "        File \"<string>\", line 2, in <module>\n",
            "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
            "        File \"C:\\Users\\Rohit\\AppData\\Local\\Temp\\pip-install-eutweux8\\pytorch_f324ffa9130c46ad8454c0712049d11d\\setup.py\", line 15, in <module>\n",
            "          raise Exception(message)\n",
            "      Exception: You tried to install \"pytorch\". The package named for PyTorch is \"torch\"\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for pytorch\n",
            "ERROR: Could not build wheels for pytorch, which is required to install pyproject.toml-based projects\n"
          ]
        }
      ],
      "source": [
        " !pip install pytorch torchvision torchaudio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5yQWioLSe8y",
        "outputId": "29e67968-cabb-414c-b187-128e47f0f87d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.3.0+cpu.html\n",
            "Collecting pyg_lib\n",
            "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcpu/pyg_lib-0.4.0%2Bpt23cpu-cp311-cp311-win_amd64.whl (717 kB)\n",
            "     ---------------------------------------- 0.0/717.9 kB ? eta -:--:--\n",
            "     -- ---------------------------------- 41.0/717.9 kB 960.0 kB/s eta 0:00:01\n",
            "     ------- ------------------------------ 143.4/717.9 kB 1.7 MB/s eta 0:00:01\n",
            "     ----------- -------------------------- 225.3/717.9 kB 1.7 MB/s eta 0:00:01\n",
            "     ---------------------- --------------- 430.1/717.9 kB 2.4 MB/s eta 0:00:01\n",
            "     -------------------------------- ----- 614.4/717.9 kB 2.8 MB/s eta 0:00:01\n",
            "     -------------------------------------- 717.9/717.9 kB 2.8 MB/s eta 0:00:00\n",
            "Collecting torch_scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcpu/torch_scatter-2.1.2%2Bpt23cpu-cp311-cp311-win_amd64.whl (336 kB)\n",
            "     ---------------------------------------- 0.0/336.7 kB ? eta -:--:--\n",
            "     -------------------------------------- 336.7/336.7 kB 6.9 MB/s eta 0:00:00\n",
            "Collecting torch_sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcpu/torch_sparse-0.6.18%2Bpt23cpu-cp311-cp311-win_amd64.whl (793 kB)\n",
            "     ---------------------------------------- 0.0/793.1 kB ? eta -:--:--\n",
            "     ---------- --------------------------- 215.0/793.1 kB 6.6 MB/s eta 0:00:01\n",
            "     ---------------------- --------------- 471.0/793.1 kB 5.9 MB/s eta 0:00:01\n",
            "     ------------------------------ ------- 634.9/793.1 kB 5.0 MB/s eta 0:00:01\n",
            "     -------------------------------------- 793.1/793.1 kB 5.0 MB/s eta 0:00:00\n",
            "Collecting torch_cluster\n",
            "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcpu/torch_cluster-1.6.3%2Bpt23cpu-cp311-cp311-win_amd64.whl (508 kB)\n",
            "     ---------------------------------------- 0.0/508.3 kB ? eta -:--:--\n",
            "     ---------------------- --------------- 307.2/508.3 kB 6.3 MB/s eta 0:00:01\n",
            "     -------------------------------------  501.8/508.3 kB 6.2 MB/s eta 0:00:01\n",
            "     -------------------------------------- 508.3/508.3 kB 5.3 MB/s eta 0:00:00\n",
            "Collecting torch_spline_conv\n",
            "  Downloading https://data.pyg.org/whl/torch-2.3.0%2Bcpu/torch_spline_conv-1.2.2%2Bpt23cpu-cp311-cp311-win_amd64.whl (178 kB)\n",
            "     ---------------------------------------- 0.0/178.6 kB ? eta -:--:--\n",
            "     ------------------------------ ------- 143.4/178.6 kB 2.8 MB/s eta 0:00:01\n",
            "     -------------------------------------- 178.6/178.6 kB 2.7 MB/s eta 0:00:00\n",
            "Requirement already satisfied: scipy in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch_sparse) (1.12.0)\n",
            "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from scipy->torch_sparse) (1.24.4)\n",
            "Installing collected packages: torch_spline_conv, torch_scatter, pyg_lib, torch_sparse, torch_cluster\n",
            "Successfully installed pyg_lib-0.4.0+pt23cpu torch_cluster-1.6.3+pt23cpu torch_scatter-2.1.2+pt23cpu torch_sparse-0.6.18+pt23cpu torch_spline_conv-1.2.2+pt23cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.3.0+cpu.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE1DOsLjTX83",
        "outputId": "9f8875e4-aa64-4549-d286-cc52038ec7a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch_geometric in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (2.5.3)\n",
            "Requirement already satisfied: tqdm in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch_geometric) (4.66.4)\n",
            "Requirement already satisfied: numpy in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch_geometric) (1.24.4)\n",
            "Requirement already satisfied: scipy in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch_geometric) (1.12.0)\n",
            "Requirement already satisfied: fsspec in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch_geometric) (2024.2.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch_geometric) (3.1.4)\n",
            "Requirement already satisfied: aiohttp in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch_geometric) (3.1.1)\n",
            "Requirement already satisfied: scikit-learn in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: psutil>=5.8.0 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from torch_geometric) (5.9.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from requests->torch_geometric) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from requests->torch_geometric) (2.2.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from scikit-learn->torch_geometric) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from scikit-learn->torch_geometric) (3.2.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\rohit\\appdata\\roaming\\python\\python311\\site-packages (from tqdm->torch_geometric) (0.4.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fODIIJG_cyvQ",
        "outputId": "61b77901-1b45-4bcc-b5c8-72d01526071f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: python-louvain in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (0.16)\n",
            "Requirement already satisfied: networkx in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from python-louvain) (3.3)\n",
            "Requirement already satisfied: numpy in c:\\users\\rohit\\anaconda3\\envs\\devenv\\lib\\site-packages (from python-louvain) (1.24.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install python-louvain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d-6hBOj1mMl",
        "outputId": "b1bd5b35-dabc-4c06-dcae-01d8c209b249"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnvMUOEzErr6",
        "outputId": "cc552738-0471-4df1-a3b0-a20774060906"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import networkx as nx\n",
        "import itertools\n",
        "from torch_geometric.utils import to_scipy_sparse_matrix, from_networkx\n",
        "from community import community_louvain\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, BatchNorm, SAGEConv, GATConv\n",
        "from torch_geometric.loader import NeighborLoader\n",
        "from torch.optim import Adam\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import importlib\n",
        "import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "# import ipdb\n",
        "# from google.colab import drive\n",
        "# drive.mount('/mnt/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<module 'metrics' from 'e:\\\\MSDS Course\\\\Spring 2024\\\\CSE 547 ML for Big Data\\\\FinalProject\\\\metrics.py'>"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "importlib.reload(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TYcNNwO6IqTj"
      },
      "outputs": [],
      "source": [
        "subreddit_hyperlinks = pd.read_csv(\"soc-redditHyperlinks-body.tsv\", delimiter=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PzxdP7d3JpPW"
      },
      "outputs": [],
      "source": [
        "subreddit_categories = pd.read_csv(\"reddit_categoriesv2.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6lGv0qErJpPY"
      },
      "outputs": [],
      "source": [
        "subreddit_categories['Names'] = subreddit_categories['Names'].astype(str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UbggOCw1JpPh"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Names</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>legaladviceinaction</td>\n",
              "      <td>Business &amp; Finance</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>pyrography</td>\n",
              "      <td>Art &amp; Design</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sexpositive</td>\n",
              "      <td>Family &amp; Relationships</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>casualpokereferences</td>\n",
              "      <td>Gaming</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>submechanophobia</td>\n",
              "      <td>Healthy Living</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21582</th>\n",
              "      <td>freeletics</td>\n",
              "      <td>Healthy Living</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21583</th>\n",
              "      <td>cosplayprops</td>\n",
              "      <td>Art &amp; Design</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21584</th>\n",
              "      <td>men</td>\n",
              "      <td>Family &amp; Relationships</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21585</th>\n",
              "      <td>paranoia</td>\n",
              "      <td>Healthy Living</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21586</th>\n",
              "      <td>52in52</td>\n",
              "      <td>Reading</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>21587 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Names                Category\n",
              "0       legaladviceinaction      Business & Finance\n",
              "1                pyrography            Art & Design\n",
              "2               sexpositive  Family & Relationships\n",
              "3      casualpokereferences                  Gaming\n",
              "4          submechanophobia          Healthy Living\n",
              "...                     ...                     ...\n",
              "21582            freeletics          Healthy Living\n",
              "21583          cosplayprops            Art & Design\n",
              "21584                   men  Family & Relationships\n",
              "21585              paranoia          Healthy Living\n",
              "21586                52in52                 Reading\n",
              "\n",
              "[21587 rows x 2 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "subreddit_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oYBpcaPXUbUc"
      },
      "outputs": [],
      "source": [
        "subreddit_embeddings = pd.read_csv(\"subreddit_embeddings.csv\", header=None)\n",
        "subreddit_embeddings.columns = ['subreddit'] + [f'embedding_{i}' for i in range(300)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JsKtQ_Y6I-7h",
        "outputId": "617ddba8-dcc8-42b8-86c7-7220bf275bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total 35776 unique subreddits from the edge hyperlinks\n"
          ]
        }
      ],
      "source": [
        "subreddits = list(set(set(subreddit_hyperlinks['SOURCE_SUBREDDIT']).union(set(subreddit_hyperlinks['TARGET_SUBREDDIT']))))\n",
        "print(f\"Total {len(subreddits)} unique subreddits from the edge hyperlinks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEptt_gcWX-j",
        "outputId": "807e7caf-934a-4ff5-993f-4ac0a37f76c6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "We have subreddit embeddings for 21206 unique subreddits\n"
          ]
        }
      ],
      "source": [
        "subreddit_names_set = set(subreddit_categories['Names'])\n",
        "fin_subreddit_names = list(set(subreddit_embeddings['subreddit']).intersection(set(subreddits)))\n",
        "fin_subreddit_names = [name for name in fin_subreddit_names if name in subreddit_names_set]\n",
        "print(f\"We have subreddit embeddings for {len(fin_subreddit_names)} unique subreddits\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1Wd9yJ1beFos"
      },
      "outputs": [],
      "source": [
        "subreddit_embeddings = subreddit_embeddings[subreddit_embeddings['subreddit'].isin(fin_subreddit_names)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE1-gkbMhvVp"
      },
      "source": [
        "## Creating subreddit name and embedding mapping"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "VNwySWhFgtX7"
      },
      "outputs": [],
      "source": [
        "subreddit_to_embedding = {row['subreddit']: row[1:].values.tolist() for _, row in subreddit_embeddings.iterrows()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXlE9XXBYi4g"
      },
      "source": [
        "## FIltering edges for only those subreddits for which we have node embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "gqmin18Ta3_I"
      },
      "outputs": [],
      "source": [
        "subreddit_hyperlinks = subreddit_hyperlinks[subreddit_hyperlinks['SOURCE_SUBREDDIT'].isin(fin_subreddit_names) & subreddit_hyperlinks['TARGET_SUBREDDIT'].isin(fin_subreddit_names) ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJ_KQaoxVziJ"
      },
      "source": [
        "## Create subreddit name vs node_id map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "wvS0rAMiVin2"
      },
      "outputs": [],
      "source": [
        "subreddit_to_id = {subreddit: idx for idx, subreddit in enumerate(fin_subreddit_names)}\n",
        "id_to_subreddit = {idx:subreddit for idx, subreddit in enumerate(fin_subreddit_names)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py86eLbbJpQy",
        "outputId": "5f875bcc-7562-4569-88aa-7217d745b758"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "21206"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(fin_subreddit_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNYqAmsOJpQ4"
      },
      "source": [
        "## Process subreddit classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ibj7Y9pJpQ5",
        "outputId": "ecef3944-bef1-4457-9f53-2da1fb9fdd86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Entertainment             4296\n",
            "Gaming                    3970\n",
            "News & Education          2660\n",
            "Technology & Computing    2249\n",
            "Travel & Geography        1243\n",
            "Healthy Living            1038\n",
            "Sports                    1023\n",
            "Music & Audio              952\n",
            "Business & Finance         893\n",
            "Art & Design               717\n",
            "Family & Relationships     585\n",
            "Food & Drink               315\n",
            "Reading                    311\n",
            "Animals & Pets             289\n",
            "Automotive                 273\n",
            "Style & Fashion            264\n",
            "Television & Film          214\n",
            "Moods                      174\n",
            "Toys & Collectibles         66\n",
            "Industries                  55\n",
            "Name: Category, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "print(subreddit_categories['Category'].value_counts())\n",
        "subreddit_categories['node_id'] = subreddit_categories['Names'].map(subreddit_to_id)\n",
        "subredditclass_to_index = {cls: idx for idx, cls in enumerate(subreddit_categories['Category'].unique())}\n",
        "\n",
        "subreddit_name_to_class = defaultdict(str)\n",
        "for _, row in subreddit_categories.iterrows():\n",
        "    subreddit_name_to_class[row['Names']]=row['Category']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmoVUGlRfuFA"
      },
      "source": [
        "## Create graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "kkMdCkUpfspg"
      },
      "outputs": [],
      "source": [
        "G = nx.Graph()\n",
        "for name in fin_subreddit_names:\n",
        "    G.add_node(subreddit_to_id[name], y=subredditclass_to_index[subreddit_name_to_class[name]])\n",
        "\n",
        "for index, row in subreddit_hyperlinks.iterrows():\n",
        "    source = subreddit_to_id[row['SOURCE_SUBREDDIT']]\n",
        "    target = subreddit_to_id[row['TARGET_SUBREDDIT']] #node_ids (indices)\n",
        "    G.add_edge(source, target)\n",
        "\n",
        "#add subreddit embeddings to nodes\n",
        "for node_id in G.nodes:\n",
        "    G.nodes[node_id]['x'] = subreddit_to_embedding[id_to_subreddit[node_id]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "HkkreTdiJpRH"
      },
      "outputs": [],
      "source": [
        "data = from_networkx(G)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh3z7lZYJpRN"
      },
      "source": [
        "## Performing Stratified split across classes to get training and testing masks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rkLw2wrRJpRN"
      },
      "outputs": [],
      "source": [
        "train_ids, test_ids = train_test_split(\n",
        "    list(G.nodes()), test_size=0.3, stratify=list(nx.get_node_attributes(G, 'y').values()), random_state=42\n",
        ")\n",
        "num_nodes = data.num_nodes\n",
        "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
        "train_mask[train_ids] = True\n",
        "test_mask[test_ids] = True\n",
        "data.train_mask = train_mask\n",
        "data.test_mask = test_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2YCIoFwmgfO"
      },
      "source": [
        "## Run Louvain on the graph to get pseudo-labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "H53lxNW-mf7z"
      },
      "outputs": [],
      "source": [
        "#partition = community_louvain.best_partition(G)\n",
        "partitions = nx.community.louvain_communities(G, resolution=1, seed=123)\n",
        "partition_dict_louvain = {node_id: i for i, community in enumerate(partitions) for node_id in community}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "YvtacJYUJpRV",
        "outputId": "3b3fce30-1afc-49e9-81d8-64929aa35e18"
      },
      "outputs": [],
      "source": [
        "partition_counts = {community:len(p) for community, p in enumerate(partitions)}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NClOiUy5IZ1K"
      },
      "source": [
        "## GraphSAGE Trial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f95R3s1RkbQ",
        "outputId": "7dde2b03-2de1-45a4-b9ae-cef944981284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, Loss: 2.978233575820923\n",
            "Epoch 20, Loss: 1.6531767845153809\n",
            "Epoch 40, Loss: 1.4821962118148804\n",
            "Epoch 60, Loss: 1.3855888843536377\n",
            "Epoch 80, Loss: 1.3282767534255981\n",
            "Epoch 100, Loss: 1.2894536256790161\n",
            "Epoch 120, Loss: 1.267521619796753\n",
            "Epoch 140, Loss: 1.2364091873168945\n",
            "Epoch 160, Loss: 1.2225289344787598\n",
            "Epoch 180, Loss: 1.2140306234359741\n",
            "Accuracy: 0.5970\n"
          ]
        }
      ],
      "source": [
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(GraphSAGE, self).__init__()\n",
        "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
        "        self.conv2 = SAGEConv(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, edge_index = data.x, data.edge_index\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = F.relu(x)\n",
        "        x = F.dropout(x, p=0.5, training=self.training)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Model parameters\n",
        "input_dim = 300\n",
        "hidden_dim = 256\n",
        "output_dim = 20\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = GraphSAGE(input_dim, hidden_dim, output_dim)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "def train(data, epochs=200):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data)\n",
        "        loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if epoch % 20 == 0:\n",
        "            print(f'Epoch {epoch}, Loss: {loss.item()}')\n",
        "\n",
        "# Test the model\n",
        "def test(data):\n",
        "    model.eval()\n",
        "    out = model(data)\n",
        "    _, pred = out[data.test_mask].max(dim=1)\n",
        "    correct = pred.eq(data.y[data.test_mask]).sum().item()\n",
        "    accuracy = correct / data.test_mask.sum().item()\n",
        "    print(f'Accuracy: {accuracy:.4f}')\n",
        "\n",
        "# Run the training and testing\n",
        "train(data)\n",
        "test(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "Fu1PmUthQFJv"
      },
      "outputs": [],
      "source": [
        "# class GraphSAGE(torch.nn.Module):\n",
        "#     def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "#         super(GraphSAGE, self).__init__()\n",
        "#         self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
        "#         self.conv2 = SAGEConv(hidden_dim, output_dim)\n",
        "\n",
        "#     def forward(self, data):\n",
        "#         x, edge_index = data.x, data.edge_index\n",
        "#         x = self.conv1(x, edge_index)\n",
        "#         x = F.relu(x)\n",
        "#         x = F.dropout(x, p=0.5, training=self.training)\n",
        "#         x = self.conv2(x, edge_index)\n",
        "#         return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-VFHhpWJpRd"
      },
      "source": [
        "### Training GCN for community classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "WZ-nxZoZJpRe"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim1, output_dim):\n",
        "        super(GCN, self).__init__()\n",
        "        self.conv1 = GCNConv(input_dim, hidden_dim1)\n",
        "        self.bn1 = BatchNorm(hidden_dim1)\n",
        "        self.conv2 = GCNConv(hidden_dim1, output_dim)\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)  # Apply BatchNorm\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x, edge_index)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "p4UQs9ObIh9z"
      },
      "outputs": [],
      "source": [
        "# class GAT(torch.nn.Module):\n",
        "#     def __init__(self, input_dim, hidden_dim, output_dim, num_heads=16):\n",
        "#         super(GAT, self).__init__()\n",
        "#         self.conv1 = GATConv(input_dim, hidden_dim, heads=num_heads, dropout=0.6)\n",
        "#         self.conv2 = GATConv(hidden_dim * num_heads, output_dim, heads=1, concat=False, dropout=0.6)\n",
        "\n",
        "#     def forward(self, x, edge_index):\n",
        "#         x = F.dropout(x, p=0.6, training=self.training)\n",
        "#         x = self.conv1(x, edge_index)\n",
        "#         x = F.elu(x)\n",
        "#         x = F.dropout(x, p=0.6, training=self.training)\n",
        "#         x = self.conv2(x, edge_index)\n",
        "#         return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "konNc2N1JpRg"
      },
      "outputs": [],
      "source": [
        "input_dim = data.x.shape[1]\n",
        "hidden_dim1 = 128\n",
        "output_dim = len(set(nx.get_node_attributes(G, 'y').values()))\n",
        "\n",
        "#model = GAT(input_dim, hidden_dim1, output_dim)\n",
        "model = GCN(input_dim, hidden_dim1, output_dim)\n",
        "optimizer = Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "IYIzs0OOO5Lt"
      },
      "outputs": [],
      "source": [
        "train_loader = NeighborLoader(\n",
        "    data,\n",
        "    num_neighbors=[500, 500],\n",
        "    batch_size=1024,\n",
        "    input_nodes=data.train_mask,\n",
        ")\n",
        "\n",
        "test_loader = NeighborLoader(\n",
        "    data,\n",
        "    num_neighbors=[500, 500],\n",
        "    batch_size=1024,\n",
        "    input_nodes=data.test_mask,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "cid86vEZJpRn"
      },
      "outputs": [],
      "source": [
        "def train():\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        loss = criterion(out, batch.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * batch.batch_size\n",
        "\n",
        "    return total_loss / len(train_loader.dataset)\n",
        "\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    totals = 0\n",
        "    for batch in loader:\n",
        "        out = model(batch.x, batch.edge_index)\n",
        "        pred = out.argmax(dim=1)\n",
        "        correct += pred.eq(batch.y).sum().item()\n",
        "        totals+=len(batch.y)\n",
        "    return correct / totals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9386Laa2JpRp",
        "outputId": "6c9f1e00-f6fa-4148-d2c4-a2e053c3c9d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 001, Loss: 2.0921\n",
            "Epoch: 002, Loss: 1.7614\n",
            "Epoch: 003, Loss: 1.6706\n",
            "Epoch: 004, Loss: 1.6085\n",
            "Epoch: 005, Loss: 1.5575\n",
            "Epoch: 006, Loss: 1.5155\n",
            "Epoch: 007, Loss: 1.4897\n",
            "Epoch: 008, Loss: 1.4577\n",
            "Epoch: 009, Loss: 1.4566\n",
            "Epoch: 010, Loss: 1.4190\n",
            "Epoch: 010, Loss: 1.4190, Test Acc: 0.5767\n",
            "Epoch: 011, Loss: 1.4071\n",
            "Epoch: 012, Loss: 1.4134\n",
            "Epoch: 013, Loss: 1.3969\n",
            "Epoch: 014, Loss: 1.3688\n",
            "Epoch: 015, Loss: 1.3601\n",
            "Epoch: 016, Loss: 1.3611\n",
            "Epoch: 017, Loss: 1.3489\n",
            "Epoch: 018, Loss: 1.3856\n",
            "Epoch: 019, Loss: 1.3426\n",
            "Epoch: 020, Loss: 1.3184\n",
            "Epoch: 020, Loss: 1.3184, Test Acc: 0.6095\n",
            "Epoch: 021, Loss: 1.3224\n",
            "Epoch: 022, Loss: 1.3256\n",
            "Epoch: 023, Loss: 1.3080\n",
            "Epoch: 024, Loss: 1.3243\n",
            "Epoch: 025, Loss: 1.3293\n",
            "Epoch: 026, Loss: 1.3109\n",
            "Epoch: 027, Loss: 1.2974\n",
            "Epoch: 028, Loss: 1.3059\n",
            "Epoch: 029, Loss: 1.3020\n",
            "Epoch: 030, Loss: 1.2826\n",
            "Epoch: 030, Loss: 1.2826, Test Acc: 0.6128\n",
            "Epoch: 031, Loss: 1.3086\n",
            "Epoch: 032, Loss: 1.2820\n",
            "Epoch: 033, Loss: 1.2962\n",
            "Epoch: 034, Loss: 1.2741\n",
            "Epoch: 035, Loss: 1.2633\n",
            "Epoch: 036, Loss: 1.2765\n",
            "Epoch: 037, Loss: 1.2860\n",
            "Epoch: 038, Loss: 1.2667\n",
            "Epoch: 039, Loss: 1.2724\n",
            "Epoch: 040, Loss: 1.2522\n",
            "Epoch: 040, Loss: 1.2522, Test Acc: 0.6285\n",
            "Epoch: 041, Loss: 1.2463\n",
            "Epoch: 042, Loss: 1.2600\n",
            "Epoch: 043, Loss: 1.2788\n",
            "Epoch: 044, Loss: 1.2502\n",
            "Epoch: 045, Loss: 1.2397\n",
            "Epoch: 046, Loss: 1.2729\n",
            "Epoch: 047, Loss: 1.2744\n",
            "Epoch: 048, Loss: 1.2486\n",
            "Epoch: 049, Loss: 1.2392\n",
            "Epoch: 050, Loss: 1.2497\n",
            "Epoch: 050, Loss: 1.2497, Test Acc: 0.6257\n",
            "Epoch: 051, Loss: 1.2835\n",
            "Epoch: 052, Loss: 1.2482\n",
            "Epoch: 053, Loss: 1.2498\n",
            "Epoch: 054, Loss: 1.2478\n",
            "Epoch: 055, Loss: 1.2436\n",
            "Epoch: 056, Loss: 1.2418\n",
            "Epoch: 057, Loss: 1.2389\n",
            "Epoch: 058, Loss: 1.2277\n",
            "Epoch: 059, Loss: 1.2410\n",
            "Epoch: 060, Loss: 1.2281\n",
            "Epoch: 060, Loss: 1.2281, Test Acc: 0.6385\n",
            "Epoch: 061, Loss: 1.2250\n",
            "Epoch: 062, Loss: 1.2458\n",
            "Epoch: 063, Loss: 1.2602\n",
            "Epoch: 064, Loss: 1.2269\n",
            "Epoch: 065, Loss: 1.2344\n",
            "Epoch: 066, Loss: 1.2323\n",
            "Epoch: 067, Loss: 1.2321\n",
            "Epoch: 068, Loss: 1.2479\n",
            "Epoch: 069, Loss: 1.2204\n",
            "Epoch: 070, Loss: 1.2182\n",
            "Epoch: 070, Loss: 1.2182, Test Acc: 0.6101\n",
            "Epoch: 071, Loss: 1.2207\n",
            "Epoch: 072, Loss: 1.2139\n",
            "Epoch: 073, Loss: 1.2025\n",
            "Epoch: 074, Loss: 1.2199\n",
            "Epoch: 075, Loss: 1.2247\n",
            "Epoch: 076, Loss: 1.2361\n",
            "Epoch: 077, Loss: 1.2086\n",
            "Epoch: 078, Loss: 1.2096\n",
            "Epoch: 079, Loss: 1.1962\n",
            "Epoch: 080, Loss: 1.2051\n",
            "Epoch: 080, Loss: 1.2051, Test Acc: 0.6406\n",
            "Epoch: 081, Loss: 1.2235\n",
            "Epoch: 082, Loss: 1.2307\n",
            "Epoch: 083, Loss: 1.2202\n",
            "Epoch: 084, Loss: 1.2301\n",
            "Epoch: 085, Loss: 1.2123\n",
            "Epoch: 086, Loss: 1.2343\n",
            "Epoch: 087, Loss: 1.2167\n",
            "Epoch: 088, Loss: 1.2244\n",
            "Epoch: 089, Loss: 1.2129\n",
            "Epoch: 090, Loss: 1.2175\n",
            "Epoch: 090, Loss: 1.2175, Test Acc: 0.6404\n",
            "Epoch: 091, Loss: 1.1913\n",
            "Epoch: 092, Loss: 1.1956\n",
            "Epoch: 093, Loss: 1.1875\n",
            "Epoch: 094, Loss: 1.2074\n",
            "Epoch: 095, Loss: 1.2140\n",
            "Epoch: 096, Loss: 1.1997\n",
            "Epoch: 097, Loss: 1.1886\n",
            "Epoch: 098, Loss: 1.1976\n",
            "Epoch: 099, Loss: 1.2010\n",
            "Epoch: 100, Loss: 1.2004\n",
            "Epoch: 100, Loss: 1.2004, Test Acc: 0.6490\n",
            "Epoch: 101, Loss: 1.2038\n",
            "Epoch: 102, Loss: 1.1906\n",
            "Epoch: 103, Loss: 1.2018\n",
            "Epoch: 104, Loss: 1.2558\n",
            "Epoch: 105, Loss: 1.2057\n",
            "Epoch: 106, Loss: 1.1906\n",
            "Epoch: 107, Loss: 1.2116\n",
            "Epoch: 108, Loss: 1.1895\n",
            "Epoch: 109, Loss: 1.1950\n",
            "Epoch: 110, Loss: 1.1953\n",
            "Epoch: 110, Loss: 1.1953, Test Acc: 0.6215\n",
            "Epoch: 111, Loss: 1.2001\n",
            "Epoch: 112, Loss: 1.2092\n",
            "Epoch: 113, Loss: 1.1890\n",
            "Epoch: 114, Loss: 1.2028\n",
            "Epoch: 115, Loss: 1.1978\n",
            "Epoch: 116, Loss: 1.1797\n",
            "Epoch: 117, Loss: 1.2105\n",
            "Epoch: 118, Loss: 1.1944\n",
            "Epoch: 119, Loss: 1.1827\n",
            "Epoch: 120, Loss: 1.2061\n",
            "Epoch: 120, Loss: 1.2061, Test Acc: 0.6364\n",
            "Epoch: 121, Loss: 1.1795\n",
            "Epoch: 122, Loss: 1.1808\n",
            "Epoch: 123, Loss: 1.1874\n",
            "Epoch: 124, Loss: 1.1861\n",
            "Epoch: 125, Loss: 1.1993\n",
            "Epoch: 126, Loss: 1.1834\n",
            "Epoch: 127, Loss: 1.1931\n",
            "Epoch: 128, Loss: 1.1795\n",
            "Epoch: 129, Loss: 1.1707\n",
            "Epoch: 130, Loss: 1.1906\n",
            "Epoch: 130, Loss: 1.1906, Test Acc: 0.6353\n",
            "Epoch: 131, Loss: 1.1868\n",
            "Epoch: 132, Loss: 1.1943\n",
            "Epoch: 133, Loss: 1.1817\n",
            "Epoch: 134, Loss: 1.1838\n",
            "Epoch: 135, Loss: 1.1927\n",
            "Epoch: 136, Loss: 1.1639\n",
            "Epoch: 137, Loss: 1.1783\n",
            "Epoch: 138, Loss: 1.1690\n",
            "Epoch: 139, Loss: 1.1949\n",
            "Epoch: 140, Loss: 1.1995\n",
            "Epoch: 140, Loss: 1.1995, Test Acc: 0.6291\n",
            "Epoch: 141, Loss: 1.1777\n",
            "Epoch: 142, Loss: 1.1846\n",
            "Epoch: 143, Loss: 1.2174\n",
            "Epoch: 144, Loss: 1.1845\n",
            "Epoch: 145, Loss: 1.1886\n",
            "Epoch: 146, Loss: 1.1790\n",
            "Epoch: 147, Loss: 1.1780\n",
            "Epoch: 148, Loss: 1.2104\n",
            "Epoch: 149, Loss: 1.1808\n",
            "Epoch: 150, Loss: 1.1596\n",
            "Epoch: 150, Loss: 1.1596, Test Acc: 0.6429\n",
            "Epoch: 151, Loss: 1.1579\n",
            "Epoch: 152, Loss: 1.2066\n",
            "Epoch: 153, Loss: 1.1863\n",
            "Epoch: 154, Loss: 1.2189\n",
            "Epoch: 155, Loss: 1.1752\n",
            "Epoch: 156, Loss: 1.1670\n",
            "Epoch: 157, Loss: 1.1859\n",
            "Epoch: 158, Loss: 1.1652\n",
            "Epoch: 159, Loss: 1.1970\n",
            "Epoch: 160, Loss: 1.1856\n",
            "Epoch: 160, Loss: 1.1856, Test Acc: 0.6432\n",
            "Epoch: 161, Loss: 1.1801\n",
            "Epoch: 162, Loss: 1.1708\n",
            "Epoch: 163, Loss: 1.1723\n",
            "Epoch: 164, Loss: 1.1632\n",
            "Epoch: 165, Loss: 1.1755\n",
            "Epoch: 166, Loss: 1.1678\n",
            "Epoch: 167, Loss: 1.1679\n",
            "Epoch: 168, Loss: 1.1628\n",
            "Epoch: 169, Loss: 1.1549\n",
            "Epoch: 170, Loss: 1.2048\n",
            "Epoch: 170, Loss: 1.2048, Test Acc: 0.6397\n",
            "Epoch: 171, Loss: 1.1865\n",
            "Epoch: 172, Loss: 1.1623\n",
            "Epoch: 173, Loss: 1.1541\n",
            "Epoch: 174, Loss: 1.1937\n",
            "Epoch: 175, Loss: 1.1746\n",
            "Epoch: 176, Loss: 1.1748\n",
            "Epoch: 177, Loss: 1.1663\n",
            "Epoch: 178, Loss: 1.1682\n",
            "Epoch: 179, Loss: 1.1884\n",
            "Epoch: 180, Loss: 1.1535\n",
            "Epoch: 180, Loss: 1.1535, Test Acc: 0.6328\n",
            "Epoch: 181, Loss: 1.1586\n",
            "Epoch: 182, Loss: 1.1705\n",
            "Epoch: 183, Loss: 1.1701\n",
            "Epoch: 184, Loss: 1.1685\n",
            "Epoch: 185, Loss: 1.1903\n",
            "Epoch: 186, Loss: 1.1619\n",
            "Epoch: 187, Loss: 1.1947\n",
            "Epoch: 188, Loss: 1.1585\n",
            "Epoch: 189, Loss: 1.1712\n",
            "Epoch: 190, Loss: 1.1992\n",
            "Epoch: 190, Loss: 1.1992, Test Acc: 0.6402\n",
            "Epoch: 191, Loss: 1.1691\n",
            "Epoch: 192, Loss: 1.1672\n",
            "Epoch: 193, Loss: 1.1731\n",
            "Epoch: 194, Loss: 1.1942\n",
            "Epoch: 195, Loss: 1.1659\n",
            "Epoch: 196, Loss: 1.1506\n",
            "Epoch: 197, Loss: 1.1685\n",
            "Epoch: 198, Loss: 1.1778\n",
            "Epoch: 199, Loss: 1.1898\n",
            "Epoch: 200, Loss: 1.1626\n",
            "Epoch: 200, Loss: 1.1626, Test Acc: 0.6558\n"
          ]
        }
      ],
      "source": [
        "epochs = 200\n",
        "best_acc=0\n",
        "for epoch in range(epochs):\n",
        "    loss = train()\n",
        "    print(f'Epoch: {epoch+1:03d}, Loss: {loss:.4f}')\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        acc = test(test_loader)\n",
        "        print(f'Epoch: {epoch+1:03d}, Loss: {loss:.4f}, Test Acc: {acc:.4f}')\n",
        "        if acc > best_acc:\n",
        "            best_acc = acc\n",
        "            torch.save(model.state_dict(), 'best_gcn_model.pth')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('best_gcn_model.pth'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GCN(\n",
              "  (conv1): GCNConv(300, 128)\n",
              "  (bn1): BatchNorm(128)\n",
              "  (conv2): GCNConv(128, 20)\n",
              ")"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### *Maximum testing test accuracy is 65.58%*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Making predictions on the entire data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: 746,\n",
              " 1: 555,\n",
              " 2: 418,\n",
              " 3: 4570,\n",
              " 4: 922,\n",
              " 5: 2899,\n",
              " 6: 1100,\n",
              " 7: 54,\n",
              " 8: 132,\n",
              " 9: 1106,\n",
              " 10: 218,\n",
              " 11: 1850,\n",
              " 12: 867,\n",
              " 13: 5101,\n",
              " 14: 226,\n",
              " 15: 204,\n",
              " 16: 21,\n",
              " 17: 10,\n",
              " 18: 205,\n",
              " 19: 2}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "_, pred = model(data.x, data.edge_index).max(dim=1)\n",
        "GCN_labels = pred.numpy()\n",
        "partition_dict_GCN={}\n",
        "for id, label in enumerate(GCN_labels):\n",
        "  partition_dict_GCN[id]=label\n",
        "  partition_GCN = [[] for _ in range (output_dim)]\n",
        "  \n",
        "for key, value in partition_dict_GCN.items():\n",
        "  partition_GCN[value].append(key)\n",
        "\n",
        "partition_counts_GCN = {community:len(p) for community, p in enumerate(partition_GCN)}\n",
        "partition_counts_GCN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Calculating Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overall community detection accuracy is 0.6369895312647363\n"
          ]
        }
      ],
      "source": [
        "print(f\"Overall community detection accuracy is {accuracy_score(data.y, GCN_labels)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### *Overall community detection accuracy (GCN) is 61.64%*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "louvain_nmi = normalized_mutual_info_score(data.y, list(partition_dict_louvain.values()))\n",
        "louvain_modularity = nx.community.modularity(G, partitions)\n",
        "louvain_coverage = metrics.calculate_coverage(G, partition_dict_louvain)\n",
        "louvain_density = metrics.calculate_density(G, partition_dict_louvain)\n",
        "louvain_clustering_coefficient = metrics.calculate_clustering_coefficient(G, partition_dict_louvain)\n",
        "louvain_conductance = metrics.calculate_conductance(G, partition_dict_louvain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "GCN_nmi = normalized_mutual_info_score(data.y, list(partition_dict_GCN.values()))\n",
        "GCN_modularity = nx.community.modularity(G, partition_GCN)\n",
        "GCN_coverage = metrics.calculate_coverage(G, partition_dict_GCN)\n",
        "GCN_density = metrics.calculate_density(G, partition_dict_GCN)\n",
        "GCN_clustering_coefficient = metrics.calculate_clustering_coefficient(G, partition_dict_GCN)\n",
        "GCN_conductance = metrics.calculate_conductance(G, partition_dict_GCN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Comparison of Louvain and GCN Metrics:\n",
            "Normalized Mutual Information (NMI): Louvain - 0.038286093841006445, GCN - 0.4120788303761107\n",
            "Modularity: Louvain - 0.44864502446078436, GCN - 0.3775401013667926\n",
            "Coverage: Louvain - 0.5827863096510243, GCN - 0.5057348436275557\n",
            "Density: Louvain - 0.8119856677874575, GCN - 0.03237956037026372\n",
            "Clustering Coefficient: Louvain - 0.014823548722451282, GCN - 0.21554470780381463\n",
            "Conductance: Louvain - 0.016033635813298596, GCN - 0.583441869229324\n"
          ]
        }
      ],
      "source": [
        "# Compare the metrics\n",
        "print(\"Comparison of Louvain and GCN Metrics:\")\n",
        "print(f\"Normalized Mutual Information (NMI): Louvain - {louvain_nmi}, GCN - {GCN_nmi}\")\n",
        "print(f\"Modularity: Louvain - {louvain_modularity}, GCN - {GCN_modularity}\")\n",
        "print(f\"Coverage: Louvain - {louvain_coverage}, GCN - {GCN_coverage}\")\n",
        "print(f\"Density: Louvain - {louvain_density}, GCN - {GCN_density}\")\n",
        "print(f\"Clustering Coefficient: Louvain - {louvain_clustering_coefficient}, GCN - {GCN_clustering_coefficient}\")\n",
        "print(f\"Conductance: Louvain - {louvain_conductance}, GCN - {GCN_conductance}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "def visualize_sampled_communities(G, partition_dict, num_samples_per_community=100, seed=None):\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "\n",
        "    num_communities = max(partition_dict.values()) + 1\n",
        "    colors = cm.rainbow(np.linspace(0, 1, num_communities))\n",
        "\n",
        "    sampled_nodes = []\n",
        "    node_colors = []\n",
        "    \n",
        "    # Iterate over communities\n",
        "    for community in range(num_communities):\n",
        "        community_nodes = [node for node, comm in partition_dict.items() if comm == community]\n",
        "        \n",
        "        sampled_nodes.extend(np.random.choice(community_nodes, size=min(num_samples_per_community, len(community_nodes)), replace=False))\n",
        "        \n",
        "        node_colors.extend([colors[community]] * min(num_samples_per_community, len(community_nodes)))\n",
        "    \n",
        "    # Create a subgraph with the sampled nodes\n",
        "    subgraph_edges = [(u, v) for u, v in G.edges() if u in sampled_nodes and v in sampled_nodes]\n",
        "    subgraph = G.edge_subgraph(subgraph_edges)\n",
        "\n",
        "    subgraph_node_colors = [node_colors[sampled_nodes.index(node)] for node in subgraph.nodes()]\n",
        "\n",
        "    # Draw the subgraph\n",
        "    pos = nx.spring_layout(subgraph)\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    nx.draw_networkx_nodes(subgraph, pos, nodelist=subgraph.nodes(), node_color=subgraph_node_colors, node_size=50)\n",
        "    nx.draw_networkx_edges(subgraph, pos, alpha=0.5)\n",
        "\n",
        "    \n",
        "    plt.title('Reddit Graph communities GCN')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_sampled_communities(G, partition_dict_louvain)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "visualize_sampled_communities(G, partition_dict_GCN)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
